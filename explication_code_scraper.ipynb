{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explication du code de scraping SeLoger v12\n",
    "\n",
    "Ce notebook explique en détail le fonctionnement du scraper SeLoger, bloc par bloc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et dépendances\n",
    "\n",
    "Cette section importe toutes les bibliothèques nécessaires au fonctionnement du scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Set, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from queue import Queue\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    TimeoutException,\n",
    "    WebDriverException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "try:\n",
    "    import undetected_chromedriver as uc\n",
    "    UNDETECTED_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UNDETECTED_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication des imports\n",
    "\n",
    "| Bibliothèque | Rôle |\n",
    "|-------------|------|\n",
    "| **selenium** | Automatisation du navigateur web pour interagir avec les pages |\n",
    "| **concurrent.futures** | Exécution parallèle avec plusieurs threads (workers) |\n",
    "| **threading (Lock)** | Gestion des verrous pour la synchronisation entre threads |\n",
    "| **queue** | File d'attente thread-safe pour les pages à réessayer |\n",
    "| **undetected_chromedriver** | Version de Chrome qui évite la détection anti-bot |\n",
    "| **re** | Expressions régulières pour extraire les données des textes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration globale\n",
    "\n",
    "Cette section définit tous les paramètres de configuration du scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de base pour la recherche\n",
    "BASE_URL = \"https://www.seloger.com/classified-search?distributionTypes=Buy&estateTypes=House,Apartment&locations=AD09FR43,AD09FR44,AD09FR45\"\n",
    "\n",
    "# Paramètres généraux\n",
    "OUTPUT_DIR = \"output\"\n",
    "MAX_RETRIES = 3\n",
    "HEADLESS = False\n",
    "PARALLEL_WORKERS = 3\n",
    "MAX_WORKERS = 10\n",
    "DEBUG_MODE = False\n",
    "MISSING_DATA_INDICATOR = \"N/A\"\n",
    "\n",
    "# Délais pour simuler un comportement humain\n",
    "DELAY_BETWEEN_LISTINGS = (0.05, 0.15)  # Entre chaque annonce\n",
    "PAGE_LOAD_WAIT = (2, 4)                 # Après chargement de page\n",
    "SCROLL_DELAY = (0.2, 0.5)               # Entre les scrolls\n",
    "LAZY_SCROLL_WAIT = (0.8, 1.5)           # Pour le lazy loading\n",
    "FINAL_WAIT_AFTER_SCROLL = (1.0, 1.8)    # Après le scroll complet\n",
    "RETRY_DELAY = (5, 10)                   # Avant un retry\n",
    "\n",
    "# Seuils de qualité\n",
    "MIN_LISTINGS_PER_PAGE = 15\n",
    "MIN_COMPLETE_DATA_RATIO = 0.5\n",
    "\n",
    "# Simulation de comportement humain\n",
    "BREAK_EVERY_N_PAGES = (8, 15)   # Pause tous les 8-15 pages\n",
    "BREAK_DURATION = (5, 15)         # Durée de la pause en secondes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication de l'URL\n",
    "\n",
    "L'URL de recherche contient plusieurs paramètres :\n",
    "- `distributionTypes=Buy` : Biens à vendre (pas de location)\n",
    "- `estateTypes=House,Apartment` : Maisons et appartements\n",
    "- `locations=AD09FR43,AD09FR44,AD09FR45` : Codes des départements Loire-Atlantique et limitrophes\n",
    "\n",
    "### Pourquoi des délais aléatoires ?\n",
    "\n",
    "Les sites web détectent les bots par leur comportement trop régulier. En utilisant des intervalles aléatoires (tuples min/max), le scraper simule un utilisateur humain qui navigue naturellement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Sélecteurs CSS et User Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Agents pour simuler différents navigateurs\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Tailles d'écran courantes\n",
    "VIEWPORT_SIZES = [\n",
    "    (1920, 1080), (1366, 768), (1536, 864), (1440, 900),\n",
    "    (1600, 900), (1280, 720), (1680, 1050),\n",
    "]\n",
    "\n",
    "# Sélecteurs CSS pour localiser les éléments HTML\n",
    "SELECTORS = {\n",
    "    'card': \"div[data-testid='serp-core-classified-card-testid']\",\n",
    "    'url': \"a[data-testid='card-mfe-covering-link-testid']\",\n",
    "    'price_container': \"div[data-testid='cardmfe-price-testid']\",\n",
    "    'keyfacts': \"div[data-testid='cardmfe-keyfacts-testid']\",\n",
    "    'address': \"div[data-testid='cardmfe-description-box-address']\",\n",
    "    'tags': \"div[data-testid='cardmfe-tag-testid']\",\n",
    "    'energy': \"span[data-testid='card-mfe-energy-performance-class']\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi ces sélecteurs ?\n",
    "\n",
    "SeLoger utilise des attributs `data-testid` pour leurs tests automatisés. Ces sélecteurs sont **plus stables** que les classes CSS qui peuvent changer lors des mises à jour du site.\n",
    "\n",
    "| Sélecteur | Élément ciblé |\n",
    "|-----------|---------------|\n",
    "| `card` | La carte complète d'une annonce |\n",
    "| `url` | Le lien vers la page détaillée |\n",
    "| `price_container` | Le bloc contenant le prix |\n",
    "| `keyfacts` | Surface, pièces, chambres, étage |\n",
    "| `address` | Adresse, ville, code postal |\n",
    "| `energy` | Classe énergétique (DPE) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Variables globales thread-safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verrous pour la synchronisation entre threads\n",
    "csv_lock = Lock()           # Protège l'écriture CSV\n",
    "scraped_urls_lock = Lock()  # Protège l'ensemble des URLs\n",
    "stats_lock = Lock()         # Protège les statistiques\n",
    "retry_queue_lock = Lock()   # Protège la file de retry\n",
    "\n",
    "# Ensemble des URLs déjà scrapées (pour éviter les doublons)\n",
    "scraped_urls: Set[str] = set()\n",
    "\n",
    "# File d'attente des pages échouées\n",
    "retry_queue: Queue = Queue()\n",
    "\n",
    "# Statistiques globales\n",
    "global_stats = {\n",
    "    'total_listings': 0,\n",
    "    'complete_listings': 0,\n",
    "    'failed_pages': set(),\n",
    "    'successful_pages': set(),\n",
    "    'pages_by_worker': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi des verrous (Lock) ?\n",
    "\n",
    "Quand plusieurs threads (workers) travaillent en parallèle, ils peuvent essayer d'accéder aux mêmes ressources en même temps. Sans verrou :\n",
    "- Deux workers pourraient écrire dans le CSV simultanément → **corruption des données**\n",
    "- L'ensemble `scraped_urls` pourrait être modifié pendant une lecture → **erreurs**\n",
    "\n",
    "Le verrou garantit qu'un seul thread accède à la ressource à la fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Gestion automatique des popups\n",
    "\n",
    "C'est l'une des parties les plus importantes du scraper. SeLoger affiche plusieurs types de popups qui bloquent l'accès au contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dismiss_all_popups(driver, worker_id: int) -> bool:\n",
    "    \"\"\"\n",
    "    Ferme automatiquement TOUS les types de popups:\n",
    "    - Consentement cookies (Usercentrics shadow DOM)\n",
    "    - Dialogues modaux\n",
    "    - Popups newsletter\n",
    "    - Autres overlays\n",
    "    \n",
    "    Retourne True si un popup a été fermé.\n",
    "    \"\"\"\n",
    "    dismissed_any = False\n",
    "    \n",
    "    # 1. POPUP COOKIES USERCENTRICS (Shadow DOM)\n",
    "    try:\n",
    "        cookie_script = \"\"\"\n",
    "            const root = document.querySelector('#usercentrics-root');\n",
    "            if (root && root.shadowRoot) {\n",
    "                const acceptBtn = root.shadowRoot.querySelector('[data-testid=\"uc-accept-all-button\"]');\n",
    "                if (acceptBtn && acceptBtn.offsetParent !== null) {\n",
    "                    acceptBtn.click();\n",
    "                    return true;\n",
    "                }\n",
    "            }\n",
    "            return false;\n",
    "        \"\"\"\n",
    "        if driver.execute_script(cookie_script):\n",
    "            logger.info(f\"Worker {worker_id}: ✓ Dismissed Usercentrics cookie popup\")\n",
    "            dismissed_any = True\n",
    "            time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Worker {worker_id}: Cookie popup check: {e}\")\n",
    "    \n",
    "    # 2. DIALOGUES MODAUX GÉNÉRIQUES\n",
    "    try:\n",
    "        dialogs = driver.find_elements(By.CSS_SELECTOR, \"[role='dialog']\")\n",
    "        for dialog in dialogs:\n",
    "            if dialog.is_displayed():\n",
    "                close_selectors = [\n",
    "                    \"button[aria-label*='close' i]\",\n",
    "                    \"button[aria-label*='fermer' i]\",\n",
    "                    \"button[class*='close']\",\n",
    "                    \"[data-testid*='close']\",\n",
    "                ]\n",
    "                for sel in close_selectors:\n",
    "                    try:\n",
    "                        close_btn = dialog.find_element(By.CSS_SELECTOR, sel)\n",
    "                        if close_btn.is_displayed():\n",
    "                            close_btn.click()\n",
    "                            dismissed_any = True\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Suite de la fonction... (voir code complet)\n",
    "    return dismissed_any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les différents types de popups gérés\n",
    "\n",
    "| Type | Technique de fermeture |\n",
    "|------|------------------------|\n",
    "| **Cookies Usercentrics** | JavaScript dans le Shadow DOM |\n",
    "| **Dialogues modaux** | Recherche de boutons \"close/fermer\" |\n",
    "| **Overlays aria-modal** | Clic à l'extérieur du modal |\n",
    "| **Popins/popups** | Recherche de boutons dans l'élément |\n",
    "| **Overlays bloquants** | Clic direct sur l'overlay |\n",
    "| **Fallback** | Touche Escape |\n",
    "\n",
    "### Qu'est-ce que le Shadow DOM ?\n",
    "\n",
    "Le Shadow DOM est une technique qui isole une partie du DOM. Selenium ne peut pas y accéder directement, d'où l'utilisation de JavaScript pour trouver et cliquer sur le bouton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Parsing des annonces\n",
    "\n",
    "Cette fonction extrait toutes les données d'une carte d'annonce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_listing(card, page_num: int, worker_id: int) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Extrait toutes les données d'une carte d'annonce.\"\"\"\n",
    "    \n",
    "    # Structure de données initiale\n",
    "    data = {\n",
    "        'page_num': page_num,\n",
    "        'type': None,           # Appartement, Maison, etc.\n",
    "        'price': None,          # Prix en euros\n",
    "        'price_per_m2': None,   # Prix au m²\n",
    "        'surface': None,        # Surface en m²\n",
    "        'rooms': None,          # Nombre de pièces\n",
    "        'bedrooms': None,       # Nombre de chambres\n",
    "        'floor': None,          # Étage\n",
    "        'address': None,        # Adresse complète\n",
    "        'city': None,           # Ville\n",
    "        'postal_code': None,    # Code postal\n",
    "        'department': None,     # Département (2 premiers chiffres)\n",
    "        'url': None,            # URL de l'annonce\n",
    "        'energy_class': None,   # Classe énergétique (A-G)\n",
    "        'is_new': False,        # Bien neuf ?\n",
    "        'agency': None,         # Agence immobilière\n",
    "        'confidence_score': 10, # Score de confiance (0-10)\n",
    "    }\n",
    "    \n",
    "    # Extraction du texte brut pour les fallbacks\n",
    "    raw_text = card.text\n",
    "    \n",
    "    # 1. EXTRACTION DE L'URL\n",
    "    try:\n",
    "        url_elem = card.find_element(By.CSS_SELECTOR, SELECTORS['url'])\n",
    "        url = url_elem.get_attribute('href')\n",
    "        if url:\n",
    "            data['url'] = url if url.startswith('http') else f\"https://www.seloger.com{url}\"\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # 2. EXTRACTION DU PRIX (avec regex)\n",
    "    try:\n",
    "        price_elem = card.find_element(By.CSS_SELECTOR, SELECTORS['price_container'])\n",
    "        price_text = price_elem.text\n",
    "        \n",
    "        # Prix au m²\n",
    "        price_m2_match = re.search(r'([\\d\\s\\u00a0\\u202f,\\.]+\\s*€\\s*/\\s*m[²2])', price_text)\n",
    "        if price_m2_match:\n",
    "            data['price_per_m2'] = price_m2_match.group(1)\n",
    "        \n",
    "        # Prix principal\n",
    "        main_price_match = re.search(r'([\\d\\s\\u00a0\\u202f]+)\\s*€(?!\\s*/)', price_text)\n",
    "        if main_price_match:\n",
    "            data['price'] = f\"{main_price_match.group(1).strip()} €\"\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # Suite... (voir code complet)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions régulières utilisées\n",
    "\n",
    "| Pattern | Données extraites | Exemple |\n",
    "|---------|-------------------|----------|\n",
    "| `(\\d+(?:[,\\.]\\d+)?)\\s*m[²2]` | Surface | \"85 m²\" → \"85\" |\n",
    "| `(\\d+)\\s*pièces?` | Nombre de pièces | \"4 pièces\" → \"4\" |\n",
    "| `(\\d+)\\s*chambres?` | Nombre de chambres | \"2 chambres\" → \"2\" |\n",
    "| `\\((\\d{5})\\)` | Code postal | \"(44000)\" → \"44000\" |\n",
    "| `([\\d\\s]+)\\s*€` | Prix | \"350 000 €\" → \"350 000\" |\n",
    "\n",
    "### Score de confiance\n",
    "\n",
    "Le score (0-10) indique la qualité des données extraites :\n",
    "- **10** : URL + Prix + Surface trouvés\n",
    "- **7** : 2 champs critiques trouvés\n",
    "- **4** : 1 champ critique trouvé\n",
    "- **1** : Aucun champ critique\n",
    "- **0** : Erreur lors du parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Configuration du navigateur Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chrome_driver(worker_id: int, headless: bool = False) -> webdriver:\n",
    "    \"\"\"Configure un navigateur Chrome pour éviter la détection.\"\"\"\n",
    "    \n",
    "    # Sélection d'un User-Agent aléatoire\n",
    "    user_agent = USER_AGENTS[worker_id % len(USER_AGENTS)]\n",
    "    \n",
    "    if UNDETECTED_AVAILABLE:\n",
    "        # Utilise undetected-chromedriver (meilleure option)\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument(f\"--user-agent={user_agent}\")\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        driver = uc.Chrome(options=options)\n",
    "    else:\n",
    "        # Fallback sur Selenium standard avec options anti-détection\n",
    "        options = ChromeOptions()\n",
    "        options.add_argument(f\"--user-agent={user_agent}\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        # Supprime la propriété webdriver\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    # Taille de fenêtre aléatoire\n",
    "    width, height = random.choice(VIEWPORT_SIZES)\n",
    "    driver.set_window_size(width, height)\n",
    "    \n",
    "    # Positionnement en grille pour voir tous les workers\n",
    "    x_offset = (worker_id % 3) * 650\n",
    "    y_offset = (worker_id // 3) * 450\n",
    "    driver.set_window_position(x_offset, y_offset)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques anti-détection\n",
    "\n",
    "| Technique | Explication |\n",
    "|-----------|-------------|\n",
    "| **User-Agent rotatif** | Chaque worker simule un navigateur différent |\n",
    "| **disable-blink-features** | Désactive les flags d'automatisation Chrome |\n",
    "| **excludeSwitches** | Supprime le switch \"enable-automation\" |\n",
    "| **navigator.webdriver** | Supprime la propriété JS qui trahit Selenium |\n",
    "| **Viewport aléatoire** | Simule différentes résolutions d'écran |\n",
    "| **undetected-chromedriver** | Version patchée de Chrome plus difficile à détecter |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Défilement intelligent pour le lazy loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_load_all_cards(driver, worker_id: int, page_num: int) -> int:\n",
    "    \"\"\"Défile la page pour charger toutes les annonces (lazy loading).\"\"\"\n",
    "    \n",
    "    scroll_steps = 5  # Nombre d'étapes de scroll\n",
    "    last_card_count = 0\n",
    "    stable_count = 0\n",
    "    \n",
    "    for step in range(scroll_steps):\n",
    "        # Calcul de la position cible\n",
    "        scroll_position = (step + 1) * (1.0 / scroll_steps)\n",
    "        target_y = int(driver.execute_script(\"return document.body.scrollHeight\") * scroll_position)\n",
    "        current_y = driver.execute_script(\"return window.pageYOffset\")\n",
    "        \n",
    "        distance = target_y - current_y\n",
    "        num_increments = random.randint(8, 15)\n",
    "        \n",
    "        # Scroll progressif avec easing cubique\n",
    "        for i in range(num_increments):\n",
    "            progress = (i + 1) / num_increments\n",
    "            eased_progress = 1 - pow(1 - progress, 3)  # Ease-out cubic\n",
    "            next_y = int(current_y + (distance * eased_progress))\n",
    "            driver.execute_script(f\"window.scrollTo({{top: {next_y}, behavior: 'smooth'}});\")\n",
    "            time.sleep(random.uniform(0.03, 0.1))\n",
    "        \n",
    "        time.sleep(random.uniform(*LAZY_SCROLL_WAIT))\n",
    "        \n",
    "        # Vérification des popups pendant le scroll\n",
    "        check_and_dismiss_popups_if_needed(driver, worker_id)\n",
    "        \n",
    "        # Compte les cartes chargées\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, SELECTORS['card'])\n",
    "        current_count = len(cards)\n",
    "        \n",
    "        # Arrêt si le nombre de cartes est stable\n",
    "        if current_count == last_card_count:\n",
    "            stable_count += 1\n",
    "            if stable_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable_count = 0\n",
    "        last_card_count = current_count\n",
    "    \n",
    "    # Retour en haut de page\n",
    "    driver.execute_script(\"window.scrollTo({top: 0, behavior: 'smooth'});\")\n",
    "    time.sleep(random.uniform(*FINAL_WAIT_AFTER_SCROLL))\n",
    "    \n",
    "    return len(driver.find_elements(By.CSS_SELECTOR, SELECTORS['card']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'easing cubique\n",
    "\n",
    "La formule `1 - pow(1 - progress, 3)` crée un mouvement naturel :\n",
    "- **Début** : le scroll démarre rapidement\n",
    "- **Fin** : le scroll ralentit progressivement\n",
    "\n",
    "C'est exactement comme un humain qui fait défiler une page avec la molette de souris.\n",
    "\n",
    "```\n",
    "Vitesse\n",
    "  │\n",
    "  │  ╭──────╮\n",
    "  │ ╱        ╲\n",
    "  │╱          ╲____\n",
    "  └──────────────────► Temps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Gestion du fichier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_csv(filename: str):\n",
    "    \"\"\"Crée le fichier CSV avec les en-têtes.\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    with open(filepath, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"Page_Number\", \"Type\", \"Price\", \"Price_Per_M2\", \"Surface_m2\",\n",
    "            \"Rooms\", \"Bedrooms\", \"Floor\", \"Address\", \"City\",\n",
    "            \"PostalCode\", \"Department\", \"Energy_Class\", \"Is_New\",\n",
    "            \"Agency\", \"URL\", \"Confidence_Score\"\n",
    "        ])\n",
    "\n",
    "\n",
    "def write_listings_to_csv(listings: List[Dict], output_file: str):\n",
    "    \"\"\"Écrit les annonces dans le CSV de manière thread-safe.\"\"\"\n",
    "    with csv_lock:  # Verrou pour éviter les conflits\n",
    "        filepath = os.path.join(OUTPUT_DIR, output_file)\n",
    "        with open(filepath, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for listing in listings:\n",
    "                writer.writerow([\n",
    "                    format_for_csv(str(listing['page_num'])),\n",
    "                    format_for_csv(listing['type']),\n",
    "                    format_for_csv(listing['price']),\n",
    "                    # ... autres champs\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage UTF-8 BOM\n",
    "\n",
    "L'encodage `utf-8-sig` ajoute un BOM (Byte Order Mark) au début du fichier. Cela permet à **Excel** d'ouvrir correctement le fichier avec les caractères français (accents, €, etc.).\n",
    "\n",
    "Sans le BOM, Excel pourrait afficher des caractères corrompus comme `Ã©` au lieu de `é`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Fonction principale du worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_scrape_pages(worker_id: int, driver: webdriver, pages: List[int], output_file: str) -> Dict:\n",
    "    \"\"\"Boucle principale de scraping pour un worker.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'listings': 0, 'complete': 0,\n",
    "        'failed_pages': [], 'successful_pages': [],\n",
    "    }\n",
    "    \n",
    "    pages_since_break = 0\n",
    "    next_break_at = random.randint(*BREAK_EVERY_N_PAGES)\n",
    "    \n",
    "    for page_idx, page_num in enumerate(pages):\n",
    "        try:\n",
    "            # Pause aléatoire tous les 8-15 pages\n",
    "            pages_since_break += 1\n",
    "            if pages_since_break >= next_break_at:\n",
    "                break_time = random.uniform(*BREAK_DURATION)\n",
    "                logger.info(f\"Worker {worker_id}: Taking a {break_time:.1f}s break...\")\n",
    "                time.sleep(break_time)\n",
    "                pages_since_break = 0\n",
    "                next_break_at = random.randint(*BREAK_EVERY_N_PAGES)\n",
    "            \n",
    "            # Chargement de la page\n",
    "            url = f\"{BASE_URL}&page={page_num}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(*PAGE_LOAD_WAIT))\n",
    "            \n",
    "            # Fermeture des popups\n",
    "            ensure_popups_dismissed(driver, worker_id)\n",
    "            \n",
    "            # Scroll pour charger les cartes\n",
    "            card_count = scroll_to_load_all_cards(driver, worker_id, page_num)\n",
    "            \n",
    "            if card_count == 0:\n",
    "                results['failed_pages'].append(page_num)\n",
    "                retry_queue.put((page_num, worker_id, 1))\n",
    "                continue\n",
    "            \n",
    "            # Parsing des annonces\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, SELECTORS['card'])\n",
    "            listings = []\n",
    "            \n",
    "            for card in cards:\n",
    "                time.sleep(random.uniform(*DELAY_BETWEEN_LISTINGS))\n",
    "                data = parse_listing(card, page_num, worker_id)\n",
    "                \n",
    "                # Évite les doublons\n",
    "                if not is_duplicate_url(data.get('url')):\n",
    "                    listings.append(data)\n",
    "            \n",
    "            # Écriture dans le CSV\n",
    "            write_listings_to_csv(listings, output_file)\n",
    "            results['listings'] += len(listings)\n",
    "            results['successful_pages'].append(page_num)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {worker_id}: Error on page {page_num}: {e}\")\n",
    "            results['failed_pages'].append(page_num)\n",
    "            retry_queue.put((page_num, worker_id, 1))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flux d'exécution d'un worker\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│           Pour chaque page              │\n",
    "├─────────────────────────────────────────┤\n",
    "│  1. Pause aléatoire (tous les 8-15p)    │\n",
    "│              ↓                          │\n",
    "│  2. Chargement de la page               │\n",
    "│              ↓                          │\n",
    "│  3. Fermeture des popups                │\n",
    "│              ↓                          │\n",
    "│  4. Scroll pour lazy loading            │\n",
    "│              ↓                          │\n",
    "│  5. Parsing des cartes                  │\n",
    "│              ↓                          │\n",
    "│  6. Filtrage des doublons               │\n",
    "│              ↓                          │\n",
    "│  7. Écriture CSV (thread-safe)          │\n",
    "│              ↓                          │\n",
    "│  8. Si échec → file de retry            │\n",
    "└─────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Système de retry des pages échouées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_pages(drivers: List[Tuple[int, webdriver]], output_file: str) -> Dict:\n",
    "    \"\"\"Réessaie les pages qui ont échoué.\"\"\"\n",
    "    \n",
    "    results = {'retried': 0, 'succeeded': 0, 'failed': []}\n",
    "    \n",
    "    if retry_queue.empty():\n",
    "        return results\n",
    "    \n",
    "    # Récupère toutes les pages à réessayer\n",
    "    pages_to_retry = []\n",
    "    while not retry_queue.empty():\n",
    "        page_num, original_worker, attempt = retry_queue.get_nowait()\n",
    "        if attempt <= MAX_RETRIES:\n",
    "            pages_to_retry.append((page_num, attempt))\n",
    "    \n",
    "    logger.info(f\"RETRY PHASE: {len(pages_to_retry)} pages to retry\")\n",
    "    time.sleep(random.uniform(*RETRY_DELAY))\n",
    "    \n",
    "    for page_num, attempt in pages_to_retry:\n",
    "        # Utilise un worker différent (aléatoire)\n",
    "        worker_id, driver = random.choice(drivers)\n",
    "        \n",
    "        try:\n",
    "            url = f\"{BASE_URL}&page={page_num}\"\n",
    "            driver.get(url)\n",
    "            # ... même logique que worker_scrape_pages\n",
    "            \n",
    "            if success:\n",
    "                results['succeeded'] += 1\n",
    "            else:\n",
    "                # Remet dans la queue si tentatives restantes\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    retry_queue.put((page_num, worker_id, attempt + 1))\n",
    "                else:\n",
    "                    results['failed'].append(page_num)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                retry_queue.put((page_num, worker_id, attempt + 1))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratégie de retry\n",
    "\n",
    "1. **Maximum 3 tentatives** par page\n",
    "2. **Délai aléatoire** avant chaque round de retry\n",
    "3. **Worker différent** pour chaque tentative (évite les problèmes de cache/cookies)\n",
    "4. **2 rounds de retry** maximum après le scraping principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Orchestration parallèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_parallel(start_page: int, end_page: int, output_file: str, num_workers: int):\n",
    "    \"\"\"Orchestre le scraping parallèle.\"\"\"\n",
    "    \n",
    "    # Initialisation\n",
    "    initialize_csv(output_file)\n",
    "    \n",
    "    # Ouverture des navigateurs\n",
    "    drivers = []\n",
    "    for i in range(num_workers):\n",
    "        driver = setup_chrome_driver(i)\n",
    "        driver.get(BASE_URL)\n",
    "        drivers.append((i, driver))\n",
    "        ensure_popups_dismissed(driver, i)\n",
    "    \n",
    "    # Distribution des pages (round-robin)\n",
    "    pages = list(range(start_page, end_page + 1))\n",
    "    pages_per_worker = []\n",
    "    for i in range(len(drivers)):\n",
    "        worker_pages = [p for j, p in enumerate(pages) if j % len(drivers) == i]\n",
    "        pages_per_worker.append(worker_pages)\n",
    "    \n",
    "    # Exécution parallèle\n",
    "    with ThreadPoolExecutor(max_workers=len(drivers)) as executor:\n",
    "        futures = []\n",
    "        for (worker_id, driver), worker_pages in zip(drivers, pages_per_worker):\n",
    "            future = executor.submit(\n",
    "                worker_scrape_pages,\n",
    "                worker_id, driver, worker_pages, output_file\n",
    "            )\n",
    "            futures.append((future, worker_id))\n",
    "        \n",
    "        # Attend la fin de tous les workers\n",
    "        for future, worker_id in futures:\n",
    "            result = future.result()\n",
    "            logger.info(f\"Worker {worker_id} finished: {result['listings']} listings\")\n",
    "    \n",
    "    # Phase de retry\n",
    "    for retry_round in range(MAX_RETRY_ROUNDS):\n",
    "        if retry_queue.empty():\n",
    "            break\n",
    "        retry_failed_pages(drivers, output_file)\n",
    "    \n",
    "    # Fermeture des navigateurs\n",
    "    for worker_id, driver in drivers:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution round-robin des pages\n",
    "\n",
    "Avec 3 workers et les pages 1-10 :\n",
    "\n",
    "| Worker 0 | Worker 1 | Worker 2 |\n",
    "|----------|----------|----------|\n",
    "| Page 1   | Page 2   | Page 3   |\n",
    "| Page 4   | Page 5   | Page 6   |\n",
    "| Page 7   | Page 8   | Page 9   |\n",
    "| Page 10  |          |          |\n",
    "\n",
    "Cette distribution équilibre la charge et évite que tous les workers accèdent aux mêmes pages consécutivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Point d'entrée principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"SeLoger Scraper v12\")\n",
    "    parser.add_argument(\"--start\", type=int, help=\"Start page\")\n",
    "    parser.add_argument(\"--end\", type=int, help=\"End page\")\n",
    "    parser.add_argument(\"--workers\", type=int, default=PARALLEL_WORKERS)\n",
    "    parser.add_argument(\"--output\", type=str)\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Mode interactif si pas d'arguments\n",
    "    if not args.start or not args.end:\n",
    "        start = int(input(\"Start page: \").strip())\n",
    "        end = int(input(\"End page: \").strip())\n",
    "        workers = int(input(f\"Workers (default {PARALLEL_WORKERS}): \").strip() or PARALLEL_WORKERS)\n",
    "    else:\n",
    "        start, end, workers = args.start, args.end, args.workers\n",
    "    \n",
    "    output_file = args.output or f\"seloger_v12_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    confirm = input(f\"Scrape pages {start}-{end} with {workers} workers? (y/n): \")\n",
    "    if confirm.lower() == 'y':\n",
    "        scrape_parallel(start, end, output_file, workers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modes d'utilisation\n",
    "\n",
    "**Ligne de commande :**\n",
    "```bash\n",
    "python scraper.py --start 1 --end 50 --workers 3 --debug\n",
    "```\n",
    "\n",
    "**Mode interactif :**\n",
    "```\n",
    "$ python scraper.py\n",
    "Start page: 1\n",
    "End page: 50\n",
    "Workers (default 3): 3\n",
    "Scrape pages 1-50 with 3 workers? (y/n): y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Résumé de l'architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                            MAIN                                     │\n",
    "│  - Parse arguments / Mode interactif                                │\n",
    "│  - Appelle scrape_parallel()                                        │\n",
    "└───────────────────────────────┬─────────────────────────────────────┘\n",
    "                                ▼\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                       SCRAPE_PARALLEL                               │\n",
    "│  - Initialise CSV                                                   │\n",
    "│  - Ouvre N navigateurs Chrome                                       │\n",
    "│  - Distribue les pages (round-robin)                                │\n",
    "│  - Lance ThreadPoolExecutor                                         │\n",
    "└───────────────────────────────┬─────────────────────────────────────┘\n",
    "                                ▼\n",
    "┌──────────────┐  ┌──────────────┐  ┌──────────────┐\n",
    "│   Worker 0   │  │   Worker 1   │  │   Worker 2   │\n",
    "│              │  │              │  │              │\n",
    "│  Pages 1,4,7 │  │  Pages 2,5,8 │  │  Pages 3,6,9 │\n",
    "│      ↓       │  │      ↓       │  │      ↓       │\n",
    "│  parse_      │  │  parse_      │  │  parse_      │\n",
    "│  listing()   │  │  listing()   │  │  listing()   │\n",
    "└──────┬───────┘  └──────┬───────┘  └──────┬───────┘\n",
    "       │                 │                 │\n",
    "       └────────────┬────┴────────────────┘\n",
    "                    ▼\n",
    "         ┌─────────────────────┐\n",
    "         │    CSV (thread-     │\n",
    "         │    safe avec Lock)  │\n",
    "         └─────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Points forts du scraper\n",
    "\n",
    "| Caractéristique | Avantage |\n",
    "|-----------------|----------|\n",
    "| **Parallélisation** | 3x plus rapide qu'un scraper séquentiel |\n",
    "| **Anti-détection** | User-agents rotatifs, délais humains, undetected-chromedriver |\n",
    "| **Robustesse** | Gestion des erreurs, système de retry, fallbacks multiples |\n",
    "| **Gestion des popups** | Fermeture automatique de tous types de popups |\n",
    "| **Thread-safety** | Verrous pour éviter les corruptions de données |\n",
    "| **Score de confiance** | Permet de filtrer les données de mauvaise qualité |\n",
    "| **Compatibilité Excel** | Encodage UTF-8 BOM pour les caractères français |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
